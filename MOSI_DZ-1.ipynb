{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6b95fa",
   "metadata": {},
   "source": [
    "# Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b4ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aeeb8d",
   "metadata": {},
   "source": [
    "### Длинный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efae8eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мохнатые сизые тучи, словно разбитая стая испуганных птиц, низко несутся над морем. Пронзительный, резкий ветер с океана то сбивает их в тёмную сплошную массу, то, словно играя, разрывает и мечет, громоздя в причудливые очертания.\n",
      "\n",
      "Побелело море, зашумело непогодой. Тяжко встают свинцовые воды и, клубясь клокочущей пеной, с глухим рокотом катятся в мглистую даль. Ветер злобно роется по их косматой поверхности, далеко разнося солёные брызги. А вдоль излучистого берега колоссальным хребтом массивно поднимаются белые зубчатые груды нагромождённого на отмелях льду. Точно титаны в тяжёлой хватке накидали эти гигантские обломки.\n",
      "\n",
      "Обрываясь крутыми уступами с прибрежных высот, к самому морю хмуро надвинулся дремучий лес. Ветер гудит красными стволами вековых сосен, кренит стройные ели, качая их острыми верхушками и осыпая пушистый снег с печально поникших зелёных ветвей.\n",
      "\n",
      "Бесследно проходят седые века над молчаливой страной, а дремучий лес стоит и спокойно, сумрачно, точно в глубокой думе, качает темными вершинами. Ещё ни один его могучий ствол не упал под дерзким топором алчного лесопромышленника: топи да непроходимые болота залегли в его тёмной чаще. А там, где столетние сосны перешли в мелкий кустарник, мёртвым простором потянулась безжизненная тундра и потерялась бесконечной границей в холодной мгле низко нависшего тумана.\n"
     ]
    }
   ],
   "source": [
    "file = open('text.txt','r')\n",
    "textRu = file.read()\n",
    "print(textRu)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f05e527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shaggy gray clouds, like a broken flock of frightened birds, fly low over the sea. A piercing, sharp wind from the ocean sometimes knocks them down into a dark solid mass, sometimes, as if playing, tears and throws them, piling them up into bizarre outlines.\n",
      "\n",
      "The sea has turned white, the bad weather has become noisy. The leaden waters rise heavily and, swirling with seething foam, roll with a dull roar into the misty distance. The wind angrily rummages along their shaggy surface, carrying salty spray far and wide. And along the winding coast, white jagged heaps of ice piled up on the shallows rise massively like a colossal ridge. As if titans had thrown these gigantic fragments in a heavy grip.\n",
      "\n",
      "The dense forest, breaking off in steep ledges from the coastal heights, gloomily approaches the sea. The wind hums with the red trunks of the century-old pines, tilts the slender firs, shaking their sharp tops and showering fluffy snow from the sadly drooping green branches. Gray centuries pass without a trace over the silent country, and the dense forest stands and calmly, gloomily, as if in deep thought, shakes its dark tops. Not a single one of its mighty trunks has yet fallen under the bold axe of a greedy timber merchant: swamps and impassable marshes lie in its dark thicket. And where the century-old pines have turned into small bushes, the lifeless tundra stretches out in a dead expanse and is lost as an endless border in the cold gloom of the low-hanging fog.\n"
     ]
    }
   ],
   "source": [
    "file = open('text1.txt','r')\n",
    "textEn = file.read()\n",
    "print(textEn)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacfe3eb",
   "metadata": {},
   "source": [
    "### Короткий текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438cbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "textRu = \"Мохнатые сизые тучи\"\n",
    "textEn = \"Shaggy gray clouds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e4123",
   "metadata": {},
   "source": [
    "# Провести на любом тексте лемматизацию и стемминг (nltk, pymorphy2, pymorphy3, natasha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aae6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e016a69",
   "metadata": {},
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ecd40",
   "metadata": {},
   "source": [
    "### Русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8bc318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RuLemmatized(text):\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    tokens = nltk.word_tokenize(text, language='russian')\n",
    "    lemmas = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6993033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['мохнатый', 'сизый', 'туча']\n"
     ]
    }
   ],
   "source": [
    "lemmaTextRu = RuLemmatized(textRu)\n",
    "print(lemmaTextRu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a73b26",
   "metadata": {},
   "source": [
    "### Английский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ce2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EngLemmatized(text):\n",
    "    morph = pymorphy3.MorphAnalyzer()\n",
    "    tokens = nltk.word_tokenize(text, language='english')\n",
    "    lemmas = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d950b61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shaggy', 'gray', 'clouds']\n"
     ]
    }
   ],
   "source": [
    "lemmaTextEng = EngLemmatized(textEn)\n",
    "print(lemmaTextEng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99ecac",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61c499",
   "metadata": {},
   "source": [
    "### Русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37cdb4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StemmerRu(text):\n",
    "    stemmer = SnowballStemmer(\"russian\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f09b488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['мохнат', 'сиз', 'туч']\n"
     ]
    }
   ],
   "source": [
    "stemmedTextRu = StemmerRu(textRu)\n",
    "print(stemmedTextRu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff7ac2",
   "metadata": {},
   "source": [
    "### Английский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26760fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StemmerEng(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "574014e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shaggi', 'gray', 'cloud']\n"
     ]
    }
   ],
   "source": [
    "stemmedTextEng = StemmerEng(textEn)\n",
    "print(stemmedTextEng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7e0a8",
   "metadata": {},
   "source": [
    "# Написать функцию для токенизации всех символов из ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9311b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CharacterTokenization(text):\n",
    "    text = text.lower()\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4587",
   "metadata": {},
   "source": [
    "### Русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10e0f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['м', 'о', 'х', 'н', 'а', 'т', 'ы', 'е', ' ', 'с', 'и', 'з', 'ы', 'е', ' ', 'т', 'у', 'ч', 'и']\n"
     ]
    }
   ],
   "source": [
    "chTokenTextRu = CharacterTokenization(textRu)\n",
    "print(chTokenTextRu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e344316",
   "metadata": {},
   "source": [
    "### Английский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fd45ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'h', 'a', 'g', 'g', 'y', ' ', 'g', 'r', 'a', 'y', ' ', 'c', 'l', 'o', 'u', 'd', 's']\n"
     ]
    }
   ],
   "source": [
    "chTokenTextEng = CharacterTokenization(textEn)\n",
    "print(chTokenTextEng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34326e",
   "metadata": {},
   "source": [
    "# Написать функцию для векторизации всех символов из ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "915a8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterVectorizer(text):\n",
    "    text = text.lower()\n",
    "    tokens = list(set(list((text))))\n",
    "    nums = [ text.count(token) for token in tokens ]\n",
    "    return dict(list(zip(tokens, nums)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb117e",
   "metadata": {},
   "source": [
    "### Русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5f9a5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'м': 1,\n",
       " 'з': 1,\n",
       " 'у': 1,\n",
       " 'и': 2,\n",
       " 'е': 2,\n",
       " 'х': 1,\n",
       " 'а': 1,\n",
       " ' ': 2,\n",
       " 'ы': 2,\n",
       " 'с': 1,\n",
       " 'н': 1,\n",
       " 'т': 2,\n",
       " 'о': 1,\n",
       " 'ч': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectTextRu = characterVectorizer(textRu)\n",
    "vectTextRu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45782beb",
   "metadata": {},
   "source": [
    "### Английский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bdd407a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 1,\n",
       " 'y': 2,\n",
       " 'r': 1,\n",
       " 'd': 1,\n",
       " ' ': 2,\n",
       " 'u': 1,\n",
       " 'a': 2,\n",
       " 'l': 1,\n",
       " 's': 2,\n",
       " 'c': 1,\n",
       " 'g': 3,\n",
       " 'h': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectTextEng = characterVectorizer(textEn)\n",
    "vectTextEng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5daea75",
   "metadata": {},
   "source": [
    "# Провести токенизацию и векторизацию текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77029bd",
   "metadata": {},
   "source": [
    "## после лемматизации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e066124e",
   "metadata": {},
   "source": [
    "### Русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "057ae731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мохнатый сизый туча'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textRu2 = ' '.join(lemmaTextRu)\n",
    "textRu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3b50a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['м',\n",
       " 'о',\n",
       " 'х',\n",
       " 'н',\n",
       " 'а',\n",
       " 'т',\n",
       " 'ы',\n",
       " 'й',\n",
       " ' ',\n",
       " 'с',\n",
       " 'и',\n",
       " 'з',\n",
       " 'ы',\n",
       " 'й',\n",
       " ' ',\n",
       " 'т',\n",
       " 'у',\n",
       " 'ч',\n",
       " 'а']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenTextRu = CharacterTokenization(textRu2)\n",
    "chTokenTextRu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09c5f136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'м': 1,\n",
       " 'з': 1,\n",
       " 'у': 1,\n",
       " 'и': 1,\n",
       " 'х': 1,\n",
       " 'а': 2,\n",
       " ' ': 2,\n",
       " 'й': 2,\n",
       " 'ы': 2,\n",
       " 'с': 1,\n",
       " 'н': 1,\n",
       " 'т': 2,\n",
       " 'о': 1,\n",
       " 'ч': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectTextRu = characterVectorizer(textRu2)\n",
    "vectTextRu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a42f5",
   "metadata": {},
   "source": [
    "### Английский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b52c5ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shaggy gray clouds'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textEng2 = ' '.join(lemmaTextEng)\n",
    "textEng2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14c6d29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s',\n",
       " 'h',\n",
       " 'a',\n",
       " 'g',\n",
       " 'g',\n",
       " 'y',\n",
       " ' ',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 'c',\n",
       " 'l',\n",
       " 'o',\n",
       " 'u',\n",
       " 'd',\n",
       " 's']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenTextEng = CharacterTokenization(textEng2)\n",
    "chTokenTextEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da5fa3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 1,\n",
       " 'y': 2,\n",
       " 'r': 1,\n",
       " 'd': 1,\n",
       " ' ': 2,\n",
       " 'u': 1,\n",
       " 'a': 2,\n",
       " 'l': 1,\n",
       " 's': 2,\n",
       " 'c': 1,\n",
       " 'g': 3,\n",
       " 'h': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectTextEng = characterVectorizer(textEng2)\n",
    "vectTextEng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f03e3",
   "metadata": {},
   "source": [
    "## после стемминга"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4bcef",
   "metadata": {},
   "source": [
    "### Русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61de5a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мохнат сиз туч'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textRu2 = ' '.join(stemmedTextRu)\n",
    "textRu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9baec83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['м', 'о', 'х', 'н', 'а', 'т', ' ', 'с', 'и', 'з', ' ', 'т', 'у', 'ч']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenTextRu = CharacterTokenization(textRu2)\n",
    "chTokenTextRu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e8d1c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'м': 1,\n",
       " 'з': 1,\n",
       " 'у': 1,\n",
       " 'и': 1,\n",
       " 'х': 1,\n",
       " 'а': 1,\n",
       " ' ': 2,\n",
       " 'с': 1,\n",
       " 'н': 1,\n",
       " 'т': 2,\n",
       " 'о': 1,\n",
       " 'ч': 1}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectTextRu = characterVectorizer(textRu2)\n",
    "vectTextRu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b19765",
   "metadata": {},
   "source": [
    "### Английский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa336a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shaggi gray cloud'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textEng2 = ' '.join(stemmedTextEng)\n",
    "textEng2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a792e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s',\n",
       " 'h',\n",
       " 'a',\n",
       " 'g',\n",
       " 'g',\n",
       " 'i',\n",
       " ' ',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 'c',\n",
       " 'l',\n",
       " 'o',\n",
       " 'u',\n",
       " 'd']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenTextEng = CharacterTokenization(textEng2)\n",
    "chTokenTextEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2980e3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 1,\n",
       " 'y': 1,\n",
       " 'r': 1,\n",
       " 'd': 1,\n",
       " ' ': 2,\n",
       " 'u': 1,\n",
       " 'a': 2,\n",
       " 'l': 1,\n",
       " 's': 1,\n",
       " 'c': 1,\n",
       " 'i': 1,\n",
       " 'g': 3,\n",
       " 'h': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectTextEng = characterVectorizer(textEng2)\n",
    "vectTextEng"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
