{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d290e057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ------------------- ---------------- 524.3/981.5 kB 311.0 kB/s eta 0:00:02\n",
      "     ---------------------------- ------- 786.4/981.5 kB 493.7 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 786.4/981.5 kB 493.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 981.5/981.5 kB 474.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in d:\\program files\\anaconda\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=0a7c8d3db750ca229aee19fc23be1009fc1355ce1f349051a0b1ad6c3e3f86f3\n",
      "  Stored in directory: c:\\users\\thunderobot\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "902ad7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb639c",
   "metadata": {},
   "source": [
    "# Достаём текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8e9a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There lived a squirrel in the old forest. The squirrel had a daughter, a squirrel, in the spring.\n",
      "\n",
      "Once, a squirrel and a squirrel collected mushrooms for the winter. Suddenly, a marten appeared on a nearby Christmas tree. She got ready to grab the squirrel. The mother squirrel jumped towards the marten and shouted to her daughter: \"Run!\"\n",
      "\n",
      "The squirrel took off running. Finally, she stopped. I looked around, but the places were unfamiliar! There is no squirrel mom. What to do?\n",
      "\n",
      "A squirrel saw a hollow in a pine tree, hid and fell asleep. And in the morning, mom found her daughter.\n"
     ]
    }
   ],
   "source": [
    "file = open('text1.txt','r')\n",
    "text1 = file.read()\n",
    "print(text1)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "164fb586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Жила в старом лесу белка. У белки весной появилась дочка белочка.\n",
      "\n",
      "Один раз белка с белочкой собирали грибы на зиму. Вдруг на соседней ёлке появилась куница. Она приготовилась схватить белочку. Мама – белка прыгнула навстречу кунице и крикнула дочке: «Беги!»\n",
      "\n",
      "Белочка бросилась наутёк. Наконец она остановилась. Посмотрела по сторонам, а места незнакомые! Мамы – белки нет. Что делать?\n",
      "\n",
      "Увидела белочка дупло на сосне, спряталась и заснула. А утром мама дочку нашла.\n"
     ]
    }
   ],
   "source": [
    "file = open('text.txt','r')\n",
    "text2 = file.read()\n",
    "print(text2)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82add10b",
   "metadata": {},
   "source": [
    "#  Делаем GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e21be49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT:\n",
    "    def __init__(self, embedding_dim=16, hidden_dim=32):\n",
    "        # подготовка тектса\n",
    "        self.morph_ru = MorphAnalyzer()\n",
    "        self.stopwords_ru = set(stopwords.words('russian'))\n",
    "        self.stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "        # задаём параметры модели\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # заготовки для будущих данных\n",
    "        self.vocab = {}\n",
    "        self.idx2token = {}\n",
    "        self.embeddings = None\n",
    "        self.W_q = None\n",
    "        self.W_k = None\n",
    "        self.W_v = None\n",
    "        self.W_out = None\n",
    "\n",
    "    def detect_language(self, text): # функция для определения языка\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return 'unknown'\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        lang = self.detect_language(text)\n",
    "        tokens = text.lower().split()\n",
    "        result = []\n",
    "\n",
    "        if lang == 'ru':\n",
    "            for token in tokens:\n",
    "                if token.isalpha() and token not in self.stopwords_ru:\n",
    "                    norm = self.morph_ru.parse(token)[0].normal_form\n",
    "                    result.append(norm)\n",
    "        elif lang == 'en':\n",
    "            for token in tokens:\n",
    "                if token.isalpha() and token not in self.stopwords_en:\n",
    "                    result.append(token)\n",
    "        else:\n",
    "            result = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def build_vocab(self, tokens):\n",
    "        self.vocab = {token: i for i, token in enumerate(set(tokens))}\n",
    "        self.idx2token = {i: token for token, i in self.vocab.items()}\n",
    "\n",
    "        vocab_size = len(self.vocab)\n",
    "        self.embeddings = np.random.randn(vocab_size, self.embedding_dim)\n",
    "        self.W_q = np.random.randn(self.embedding_dim, self.hidden_dim)\n",
    "        self.W_k = np.random.randn(self.embedding_dim, self.hidden_dim)\n",
    "        self.W_v = np.random.randn(self.embedding_dim, self.hidden_dim)\n",
    "        self.W_out = np.random.randn(self.hidden_dim, vocab_size)\n",
    "\n",
    "    def tokenize(self, tokens):\n",
    "        return [self.vocab[token] for token in tokens if token in self.vocab]\n",
    "\n",
    "    def embed(self, token_ids):\n",
    "        return self.embeddings[token_ids]\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def attention(self, x):\n",
    "        Q = x @ self.W_q\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "\n",
    "        seq_len = Q.shape[0]\n",
    "        scores = Q @ K.T / np.sqrt(self.hidden_dim)\n",
    "\n",
    "        # Маскирование будущих токенов\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)\n",
    "        scores[mask] = -np.inf\n",
    "\n",
    "        weights = self.softmax(scores)\n",
    "        attended = weights @ V\n",
    "        return attended\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embed(token_ids)\n",
    "        attended = self.attention(x)\n",
    "        logits = attended @ self.W_out\n",
    "        return logits\n",
    "\n",
    "    def predict_next(self, token_ids):\n",
    "        logits = self.forward(token_ids)\n",
    "        probs = self.softmax(logits[-1])\n",
    "        return np.argmax(probs)\n",
    "\n",
    "    def train(self, text, epochs=10, lr=0.01):\n",
    "        tokens = self.preprocess_text(text)\n",
    "        self.build_vocab(tokens)\n",
    "        token_ids = self.tokenize(tokens)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i in range(1, len(token_ids)):\n",
    "                context = token_ids[:i]\n",
    "                target = token_ids[i]\n",
    "\n",
    "                logits = self.forward(context)\n",
    "                pred = self.softmax(logits[-1])\n",
    "                loss = -np.log(pred[target] + 1e-9)\n",
    "                total_loss += loss\n",
    "\n",
    "                # Простейшее обновление — только выходной слой\n",
    "                grad = pred\n",
    "                grad[target] -= 1\n",
    "\n",
    "                attended = self.attention(self.embed(context))\n",
    "                self.W_out -= lr * np.outer(attended[-1], grad)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    def generate(self, prompt, length=5):\n",
    "        tokens = self.preprocess_text(prompt)\n",
    "        token_ids = self.tokenize(tokens)\n",
    "        if not token_ids:\n",
    "            return \"Невозможно сгенерировать текст: нет известных токенов.\"\n",
    "\n",
    "        result = token_ids[:]\n",
    "        for _ in range(length):\n",
    "            next_id = self.predict_next(result)\n",
    "            result.append(next_id)\n",
    "\n",
    "        return ' '.join(self.idx2token[i] for i in result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff0d47b",
   "metadata": {},
   "source": [
    "## Тестрирование на коротком тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5828d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = MiniGPT(embedding_dim=16, hidden_dim=32)\n",
    "\n",
    "print(\"\\n== English ==\")\n",
    "text = \"There lived a squirrel in the old forest. The squirrel had a daughter, a squirrel, in the spring.\"\n",
    "gpt.train(text, epochs=20, lr=0.01)\n",
    "print(gpt.generate(\"daughter\", length=5))\n",
    "\n",
    "print(\"\\n== Russian ==\")\n",
    "text = \"Жила в старом лесу белка. У белки весной появилась дочка белочка.\"\n",
    "gpt.train(text, epochs=20, lr=0.01)\n",
    "print(gpt.generate(\"белка\", length=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7fde6",
   "metadata": {},
   "source": [
    "## Тестрирование на длинном  тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f2ce63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== English ==\n",
      "Epoch 1/20, Loss: 548.8243\n",
      "Epoch 2/20, Loss: 432.9416\n",
      "Epoch 3/20, Loss: 373.1243\n",
      "Epoch 4/20, Loss: 328.1320\n",
      "Epoch 5/20, Loss: 281.7741\n",
      "Epoch 6/20, Loss: 248.1675\n",
      "Epoch 7/20, Loss: 214.4786\n",
      "Epoch 8/20, Loss: 182.2440\n",
      "Epoch 9/20, Loss: 156.4731\n",
      "Epoch 10/20, Loss: 145.6014\n",
      "Epoch 11/20, Loss: 136.8998\n",
      "Epoch 12/20, Loss: 134.1483\n",
      "Epoch 13/20, Loss: 127.6996\n",
      "Epoch 14/20, Loss: 125.6631\n",
      "Epoch 15/20, Loss: 121.8354\n",
      "Epoch 16/20, Loss: 119.0474\n",
      "Epoch 17/20, Loss: 115.6667\n",
      "Epoch 18/20, Loss: 112.8146\n",
      "Epoch 19/20, Loss: 109.8517\n",
      "Epoch 20/20, Loss: 107.1506\n",
      "mother jumped shouted ready shouted mom\n",
      "\n",
      "== Russian ==\n",
      "Epoch 1/20, Loss: 665.0287\n",
      "Epoch 2/20, Loss: 646.0088\n",
      "Epoch 3/20, Loss: 582.7609\n",
      "Epoch 4/20, Loss: 512.5512\n",
      "Epoch 5/20, Loss: 457.0765\n",
      "Epoch 6/20, Loss: 392.6667\n",
      "Epoch 7/20, Loss: 335.5190\n",
      "Epoch 8/20, Loss: 293.5051\n",
      "Epoch 9/20, Loss: 251.9168\n",
      "Epoch 10/20, Loss: 217.4576\n",
      "Epoch 11/20, Loss: 185.1186\n",
      "Epoch 12/20, Loss: 161.0613\n",
      "Epoch 13/20, Loss: 146.7477\n",
      "Epoch 14/20, Loss: 137.3599\n",
      "Epoch 15/20, Loss: 130.6503\n",
      "Epoch 16/20, Loss: 124.0425\n",
      "Epoch 17/20, Loss: 118.9629\n",
      "Epoch 18/20, Loss: 116.3557\n",
      "Epoch 19/20, Loss: 114.8164\n",
      "Epoch 20/20, Loss: 113.1794\n",
      "белка соседний соседний соседний соседний соседний\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n== English ==\")\n",
    "gpt.train(text1, epochs=20, lr=0.01)\n",
    "print(gpt.generate(\"mother\", length=5))\n",
    "\n",
    "print(\"\\n== Russian ==\")\n",
    "text = \"Жила в старом лесу белка. У белки весной появилась дочка белочка.\"\n",
    "gpt.train(text2, epochs=20, lr=0.01)\n",
    "print(gpt.generate(\"белка\", length=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92b075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
