{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29c798fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     ------------------- ---------------- 524.3/981.5 kB 311.0 kB/s eta 0:00:02\n",
      "     ---------------------------- ------- 786.4/981.5 kB 493.7 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 786.4/981.5 kB 493.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 981.5/981.5 kB 474.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in d:\\program files\\anaconda\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=0a7c8d3db750ca229aee19fc23be1009fc1355ce1f349051a0b1ad6c3e3f86f3\n",
      "  Stored in directory: c:\\users\\thunderobot\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7487f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Thunderobot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a614d1",
   "metadata": {},
   "source": [
    "# Достаём текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d68681c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There lived a squirrel in the old forest. The squirrel had a daughter, a squirrel, in the spring.\n",
      "\n",
      "Once, a squirrel and a squirrel collected mushrooms for the winter. Suddenly, a marten appeared on a nearby Christmas tree. She got ready to grab the squirrel. The mother squirrel jumped towards the marten and shouted to her daughter: \"Run!\"\n",
      "\n",
      "The squirrel took off running. Finally, she stopped. I looked around, but the places were unfamiliar! There is no squirrel mom. What to do?\n",
      "\n",
      "A squirrel saw a hollow in a pine tree, hid and fell asleep. And in the morning, mom found her daughter.\n"
     ]
    }
   ],
   "source": [
    "file = open('text1.txt','r')\n",
    "text1 = file.read()\n",
    "print(text1)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a30c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Жила в старом лесу белка. У белки весной появилась дочка белочка.\n",
      "\n",
      "Один раз белка с белочкой собирали грибы на зиму. Вдруг на соседней ёлке появилась куница. Она приготовилась схватить белочку. Мама – белка прыгнула навстречу кунице и крикнула дочке: «Беги!»\n",
      "\n",
      "Белочка бросилась наутёк. Наконец она остановилась. Посмотрела по сторонам, а места незнакомые! Мамы – белки нет. Что делать?\n",
      "\n",
      "Увидела белочка дупло на сосне, спряталась и заснула. А утром мама дочку нашла.\n"
     ]
    }
   ],
   "source": [
    "file = open('text.txt','r')\n",
    "text2 = file.read()\n",
    "print(text2)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad240f73",
   "metadata": {},
   "source": [
    "#  Делаем GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91a99943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT:\n",
    "    def __init__(self, embedding_dim=16, hidden_dim=32):\n",
    "        # подготовка тектса\n",
    "        self.morph_ru = MorphAnalyzer()\n",
    "        self.stopwords_ru = set(stopwords.words('russian'))\n",
    "        self.stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "        # задаём параметры модели\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # заготовки для будущих данных\n",
    "        self.vocab = {}\n",
    "        self.idx2token = {}\n",
    "        self.embeddings = None\n",
    "        self.W_q = None\n",
    "        self.W_k = None\n",
    "        self.W_v = None\n",
    "        self.W_out = None\n",
    "\n",
    "    def detect_language(self, text): # функция для определения языка\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return 'unknown'\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        lang = self.detect_language(text)\n",
    "        tokens = text.lower().split()\n",
    "        result = []\n",
    "\n",
    "        if lang == 'ru':\n",
    "            for token in tokens:\n",
    "                if token.isalpha() and token not in self.stopwords_ru:\n",
    "                    norm = self.morph_ru.parse(token)[0].normal_form\n",
    "                    result.append(norm)\n",
    "        elif lang == 'en':\n",
    "            for token in tokens:\n",
    "                if token.isalpha() and token not in self.stopwords_en:\n",
    "                    result.append(token)\n",
    "        else:\n",
    "            result = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def build_vocab(self, tokens):\n",
    "        self.vocab = {token: i for i, token in enumerate(set(tokens))}\n",
    "        self.idx2token = {i: token for token, i in self.vocab.items()}\n",
    "\n",
    "        vocab_size = len(self.vocab)\n",
    "        self.embeddings = np.random.randn(vocab_size, self.embedding_dim)\n",
    "        self.W_q = np.random.randn(self.embedding_dim, self.hidden_dim)\n",
    "        self.W_k = np.random.randn(self.embedding_dim, self.hidden_dim)\n",
    "        self.W_v = np.random.randn(self.embedding_dim, self.hidden_dim)\n",
    "        self.W_out = np.random.randn(self.hidden_dim, vocab_size)\n",
    "\n",
    "    def tokenize(self, tokens):\n",
    "        return [self.vocab[token] for token in tokens if token in self.vocab]\n",
    "\n",
    "    def embed(self, token_ids):\n",
    "        return self.embeddings[token_ids]\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def attention(self, x):\n",
    "        Q = x @ self.W_q\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "\n",
    "        seq_len = Q.shape[0]\n",
    "        scores = Q @ K.T / np.sqrt(self.hidden_dim)\n",
    "\n",
    "        # Маскирование будущих токенов\n",
    "        mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)\n",
    "        scores[mask] = -np.inf\n",
    "\n",
    "        weights = self.softmax(scores)\n",
    "        attended = weights @ V\n",
    "        return attended\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embed(token_ids)\n",
    "        attended = self.attention(x)\n",
    "        logits = attended @ self.W_out\n",
    "        return logits\n",
    "\n",
    "    def predict_next(self, token_ids):\n",
    "        logits = self.forward(token_ids)\n",
    "        probs = self.softmax(logits[-1])\n",
    "        return np.argmax(probs)\n",
    "\n",
    "    def train(self, text, epochs=10, lr=0.01):\n",
    "        tokens = self.preprocess_text(text)\n",
    "        self.build_vocab(tokens)\n",
    "        token_ids = self.tokenize(tokens)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i in range(1, len(token_ids)):\n",
    "                context = token_ids[:i]\n",
    "                target = token_ids[i]\n",
    "\n",
    "                logits = self.forward(context)\n",
    "                pred = self.softmax(logits[-1])\n",
    "                loss = -np.log(pred[target] + 1e-9)\n",
    "                total_loss += loss\n",
    "\n",
    "                # Простейшее обновление — только выходной слой\n",
    "                grad = pred\n",
    "                grad[target] -= 1\n",
    "\n",
    "                attended = self.attention(self.embed(context))\n",
    "                self.W_out -= lr * np.outer(attended[-1], grad)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    def generate(self, prompt, length=5):\n",
    "        tokens = self.preprocess_text(prompt)\n",
    "        token_ids = self.tokenize(tokens)\n",
    "        if not token_ids:\n",
    "            return \"Невозможно сгенерировать текст: нет известных токенов.\"\n",
    "\n",
    "        result = token_ids[:]\n",
    "        for _ in range(length):\n",
    "            next_id = self.predict_next(result)\n",
    "            result.append(next_id)\n",
    "\n",
    "        return ' '.join(self.idx2token[i] for i in result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3276e",
   "metadata": {},
   "source": [
    "## Тестрирование на коротком тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d11a3f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== English ==\n",
      "Epoch 1/20, Loss: 39.2672\n",
      "Epoch 2/20, Loss: 33.1694\n",
      "Epoch 3/20, Loss: 20.9369\n",
      "Epoch 4/20, Loss: 5.5441\n",
      "Epoch 5/20, Loss: 9.8044\n",
      "Epoch 6/20, Loss: 9.7925\n",
      "Epoch 7/20, Loss: 9.7862\n",
      "Epoch 8/20, Loss: 9.7826\n",
      "Epoch 9/20, Loss: 9.7805\n",
      "Epoch 10/20, Loss: 9.7792\n",
      "Epoch 11/20, Loss: 9.7786\n",
      "Epoch 12/20, Loss: 9.7783\n",
      "Epoch 13/20, Loss: 9.7783\n",
      "Epoch 14/20, Loss: 9.7784\n",
      "Epoch 15/20, Loss: 9.7787\n",
      "Epoch 16/20, Loss: 9.7790\n",
      "Epoch 17/20, Loss: 9.7793\n",
      "Epoch 18/20, Loss: 9.7797\n",
      "Epoch 19/20, Loss: 9.7801\n",
      "Epoch 20/20, Loss: 9.7805\n",
      "squirrel old squirrel squirrel squirrel squirrel\n",
      "\n",
      "== Russian ==\n",
      "Epoch 1/20, Loss: 91.0209\n",
      "Epoch 2/20, Loss: 51.5281\n",
      "Epoch 3/20, Loss: 43.8600\n",
      "Epoch 4/20, Loss: 33.9989\n",
      "Epoch 5/20, Loss: 25.7807\n",
      "Epoch 6/20, Loss: 18.5912\n",
      "Epoch 7/20, Loss: 13.6671\n",
      "Epoch 8/20, Loss: 12.2512\n",
      "Epoch 9/20, Loss: 12.2006\n",
      "Epoch 10/20, Loss: 12.1739\n",
      "Epoch 11/20, Loss: 12.1581\n",
      "Epoch 12/20, Loss: 12.1481\n",
      "Epoch 13/20, Loss: 12.1413\n",
      "Epoch 14/20, Loss: 12.1365\n",
      "Epoch 15/20, Loss: 12.1328\n",
      "Epoch 16/20, Loss: 12.1300\n",
      "Epoch 17/20, Loss: 12.1277\n",
      "Epoch 18/20, Loss: 12.1258\n",
      "Epoch 19/20, Loss: 12.1242\n",
      "Epoch 20/20, Loss: 12.1228\n",
      "белка появиться лес весна лес весна\n"
     ]
    }
   ],
   "source": [
    "gpt = MiniGPT(embedding_dim=16, hidden_dim=32)\n",
    "\n",
    "print(\"\\n== English ==\")\n",
    "text = \"There lived a squirrel in the old forest. The squirrel had a daughter, a squirrel, in the spring.\"\n",
    "gpt.train(text, epochs=20, lr=0.01)\n",
    "print(gpt.generate(\"squirrel\", length=5))\n",
    "\n",
    "print(\"\\n== Russian ==\")\n",
    "text = \"Жила в старом лесу белка. У белки весной появилась дочка белочка.\"\n",
    "gpt.train(text, epochs=20, lr=0.01)\n",
    "print(gpt.generate(\"белка\", length=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578cdd5",
   "metadata": {},
   "source": [
    "## Тестрирование на длинном  тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2294450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== English ==\n",
      "Epoch 1/20, Loss: 540.6926\n",
      "Epoch 2/20, Loss: 453.4227\n",
      "Epoch 3/20, Loss: 383.6287\n",
      "Epoch 4/20, Loss: 328.9074\n",
      "Epoch 5/20, Loss: 293.4612\n",
      "Epoch 6/20, Loss: 276.5800\n",
      "Epoch 7/20, Loss: 253.4770\n",
      "Epoch 8/20, Loss: 228.8104\n",
      "Epoch 9/20, Loss: 211.5231\n",
      "Epoch 10/20, Loss: 199.6715\n",
      "Epoch 11/20, Loss: 191.2193\n",
      "Epoch 12/20, Loss: 185.5043\n",
      "Epoch 13/20, Loss: 181.4667\n",
      "Epoch 14/20, Loss: 180.8881\n",
      "Epoch 15/20, Loss: 179.5662\n",
      "Epoch 16/20, Loss: 179.2170\n",
      "Epoch 17/20, Loss: 178.7917\n",
      "Epoch 18/20, Loss: 178.2960\n",
      "Epoch 19/20, Loss: 177.4264\n",
      "Epoch 20/20, Loss: 176.0495\n",
      "mother mom mom mom mom mom\n",
      "\n",
      "== Russian ==\n",
      "Epoch 1/20, Loss: 653.3766\n",
      "Epoch 2/20, Loss: 542.2244\n",
      "Epoch 3/20, Loss: 478.5261\n",
      "Epoch 4/20, Loss: 432.5617\n",
      "Epoch 5/20, Loss: 392.0607\n",
      "Epoch 6/20, Loss: 343.9078\n",
      "Epoch 7/20, Loss: 322.8858\n",
      "Epoch 8/20, Loss: 313.9514\n",
      "Epoch 9/20, Loss: 295.8622\n",
      "Epoch 10/20, Loss: 279.1888\n",
      "Epoch 11/20, Loss: 263.0227\n",
      "Epoch 12/20, Loss: 247.3556\n",
      "Epoch 13/20, Loss: 233.5054\n",
      "Epoch 14/20, Loss: 217.3333\n",
      "Epoch 15/20, Loss: 203.1237\n",
      "Epoch 16/20, Loss: 196.7656\n",
      "Epoch 17/20, Loss: 192.5989\n",
      "Epoch 18/20, Loss: 187.4312\n",
      "Epoch 19/20, Loss: 187.9405\n",
      "Epoch 20/20, Loss: 184.4907\n",
      "белка появиться посмотреть место прыгнуть место\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n== English ==\")\n",
    "gpt.train(text1, epochs=20, lr=0.01)\n",
    "print(gpt.generate(\"mother\", length=5))\n",
    "\n",
    "print(\"\\n== Russian ==\")\n",
    "text = \"Жила в старом лесу белка. У белки весной появилась дочка белочка.\"\n",
    "gpt.train(text2, epochs=20, lr=0.01)\n",
    "print(gpt.generate(\"белка\", length=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6d209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
